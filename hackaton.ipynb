{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    stop_words=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_articles = \"screenplay_data\\data/raw_text_lemmas/raw_text_lemmas\"\n",
    "articles = [os.path.join(path_to_articles, item) \n",
    "            for item in os.listdir(path_to_articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_texts = []\n",
    "titles = []\n",
    "\n",
    "for article_path in articles:\n",
    "    titles.append(article_path)\n",
    "    with open(article_path, \"r\", encoding=\"utf-8\") as a_src:\n",
    "        # первые 6 строк — метаданные, последняя — число комментариев\n",
    "        articles_texts.append(\"\\n\".join(a_src.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1169/2858 [08:20<37:22,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well, trouble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1901/2858 [14:31<49:19,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well, trouble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2858/2858 [22:48<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "articles_preprocessed = []\n",
    "for a_text in tqdm(articles_texts):\n",
    "    # for lett in a_text:\n",
    "    #     if lett.isalpha():\n",
    "    #         continue\n",
    "    #     else:\n",
    "    #         a_text.replace(lett, '')\n",
    "\n",
    "    try:\n",
    "        a_tokens = wordpunct_tokenize(a_text)\n",
    "        a_lemmatized = \" \".join([morph.parse(item)[0].normal_form for item in a_tokens])\n",
    "        articles_preprocessed.append(a_lemmatized)\n",
    "    except:\n",
    "        print(\"well, trouble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "articles_preprocessed_another = []\n",
    "for a_text in tqdm(articles_texts[:20]):\n",
    "    # for lett in a_text:\n",
    "    #     if lett.isalpha():\n",
    "    #         continue\n",
    "    #     else:\n",
    "    #         a_text.replace(lett, '')\n",
    "\n",
    "    try:\n",
    "        a_tokens = wordpunct_tokenize(a_text)\n",
    "        a_lemmatized = \" \".join([morph.parse(item)[0].normal_form for item in a_tokens])\n",
    "        articles_preprocessed_another.append(a_lemmatized)\n",
    "    except:\n",
    "        print(\"well, trouble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица на 2856 документов и 338738 термов\n"
     ]
    }
   ],
   "source": [
    "articles_tfidf = tfidf.fit_transform(articles_preprocessed)\n",
    "print(f\"Матрица на {articles_tfidf.shape[0]} документов и {articles_tfidf.shape[1]} термов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица на 20 документов и 17843 термов\n"
     ]
    }
   ],
   "source": [
    "articles_tfidf_another = tfidf.fit_transform(articles_preprocessed_another)\n",
    "print(f\"Матрица на {articles_tfidf_another.shape[0]} документов и {articles_tfidf_another.shape[1]} термов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_top_tf_idf_words(tfidf_vector, feature_names, top_n):\n",
    "    sorted_nzs = np.argsort(tfidf_vector.data)[:-(top_n+1):-1]\n",
    "    return feature_names[tfidf_vector.indices[sorted_nzs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['michelle' 'howard' 'nate' 'hazmat' 'door' 'room' 'crutch' 'cont' 'int'\n",
      " 'look']\n",
      "['kat' 'bianca' 'patrick' 'cameron' 'mandella' 'joey' 'michael' 'walter'\n",
      " 'chastity' 'perky']\n",
      "['jasper' 'pongo' 'anita' 'horace' 'cruella' 'perdy' 'cu' 'roger' 'kipper'\n",
      " 'puppy']\n",
      "['juror' '8th' '3rd' 'foreman' '4th' 'th' '7th' '11th' '9th' '6th']\n",
      "['railly' 'cole' 'jeffrey' 'mason' 'dr' 'astrophysicist' 'fale' 'james'\n",
      " 'look' 'microbiologist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "for i, article in enumerate(articles_texts):\n",
    "    # напечатаю только первые 5 статей\n",
    "    if i < 5:\n",
    "        article_vector = articles_tfidf[i, :]\n",
    "        words = get_top_tf_idf_words(article_vector, feature_names, 10)\n",
    "        # print(article)\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь по хорошему надо кластеризовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey Chirkin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10, random_state=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = articles_tfidf_another\n",
    "feature_names = tfidf.get_feature_names()\n",
    "model = KMeans(n_clusters=10, random_state=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey Chirkin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_cluster.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        f.write(f\"Cluster {i}\\n\")\n",
    "        for j in order_centroids[i, :10]:\n",
    "            f.write(' %s' % terms[j])\n",
    "            f.write('\\n')\n",
    "        f.write('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c5d224b06a2df076cfd0e2012fd0909cba35e9bdf019539cd77e5b55029db13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
